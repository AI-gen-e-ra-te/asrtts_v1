import os
from openai import AsyncOpenAI
from dotenv import load_dotenv

# åŠ è½½ .env ç¯å¢ƒå˜é‡
load_dotenv()

# ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼Œæä¾›é»˜è®¤å€¼
BASE_URL = os.getenv("LLM_BASE_URL")
API_KEY = os.getenv("LLM_API_KEY")
MODEL = os.getenv("LLM_MODEL")

# åˆå§‹åŒ–å¼‚æ­¥å®¢æˆ·ç«¯
client = AsyncOpenAI(
    api_key=API_KEY,
    base_url=BASE_URL,
)

print(f"ğŸ§  LLM Client initialized: {MODEL} @ {BASE_URL}")

async def chat_stream(prompt: str):
    """
    å¼‚æ­¥ç”Ÿæˆå™¨ï¼šæµå¼è¿”å› LLM çš„æ–‡æœ¬å›å¤
    """
    try:
        response = await client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": "You are a helpful voice assistant. Please keep your replies concise, short, and conversational suitable for TTS."},
                {"role": "user", "content": prompt}
            ],
            stream=True,
            temperature=0.7,
        )

        # é€å—è¯»å–æµ
        async for chunk in response:
            content = chunk.choices[0].delta.content
            if content:
                yield content

    except Exception as e:
        print(f"âŒ LLM Error: {e}")
        yield f" Error: {str(e)}"